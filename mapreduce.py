from typing import Annotated, List, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.constants import Send
import operator
import re

# å®šä¹‰æ•´ä½“çŠ¶æ€ç»“æ„ä½“
class OverallState(TypedDict):
    # åŸå§‹å¤§è§„æ¨¡è¾“å…¥æ•°æ®
    large_input_data: List[str]
    # åˆ†å‰²åçš„å­æ•°æ®é›†
    sub_datasets: List[List[str]]
    # Map é˜¶æ®µçš„å¤„ç†ç»“æœ (ä½¿ç”¨ operator.add Reducer æ”¶é›†ç»“æœ)
    intermediate_results: Annotated[List[dict], operator.add]
    # Reduce é˜¶æ®µçš„æœ€ç»ˆç»“æœ
    final_result: dict

# å®šä¹‰ Map èŠ‚ç‚¹çš„ç§æœ‰çŠ¶æ€ç»“æ„ä½“
class MapState(TypedDict):
    sub_data: Any  # å­ä»»åŠ¡æ•°æ®ç±»å‹å¯ä»¥æ˜¯ä»»æ„ç±»å‹

def split_large_data(input_data: List[str], num_sub_tasks: int = 10) -> List[List[str]]:
    """å°†å¤§è§„æ¨¡æ•°æ®åˆ†å‰²æˆå­æ•°æ®é›†"""
    chunk_size = max(1, len(input_data) // num_sub_tasks)
    chunks = []
    for i in range(0, len(input_data), chunk_size):
        chunks.append(input_data[i:i + chunk_size])
    return chunks

def split_input_data(state: OverallState):
    """åˆ†å‰²èŠ‚ç‚¹å‡½æ•°ï¼šåªè´Ÿè´£æ•°æ®åˆ†å‰²ï¼Œä¸è¿”å› Send å¯¹è±¡"""
    input_data = state["large_input_data"]  # ä»çŠ¶æ€ä¸­è·å–å¤§è§„æ¨¡è¾“å…¥æ•°æ®
    sub_datasets = split_large_data(input_data, num_sub_tasks=4)  # å°†å¤§è§„æ¨¡æ•°æ®åˆ†å‰²æˆå­æ•°æ®é›†

    print(f"ğŸ”„ åˆ†å‰²èŠ‚ç‚¹: å°† {len(input_data)} ä¸ªæ–‡æ¡£åˆ†å‰²æˆ {len(sub_datasets)} ä¸ªå­æ•°æ®é›†")
    for i, sub_dataset in enumerate(sub_datasets):
        print(f"ğŸ“¦ å­æ•°æ®é›† {i}: {len(sub_dataset)} ä¸ªæ–‡æ¡£")

    return {"sub_datasets": sub_datasets}

def route_to_map_nodes(state: OverallState):
    """è·¯ç”±å‡½æ•°ï¼šæ ¹æ®åˆ†å‰²çš„æ•°æ®åˆ›å»º Send å¯¹è±¡"""
    sub_datasets = state["sub_datasets"]

    print(f"ğŸ”€ è·¯ç”±å‡½æ•°: åˆ›å»º {len(sub_datasets)} ä¸ªå¹¶è¡Œä»»åŠ¡")

    send_list = []
    for i, sub_dataset in enumerate(sub_datasets):  # éå†æ¯ä¸ªå­æ•°æ®é›†
        send_list.append(
            Send("map_node", {"sub_data": sub_dataset})  # ä¸ºæ¯ä¸ªå­æ•°æ®é›†åˆ›å»ºä¸€ä¸ª Send å¯¹è±¡
        )

    print(f"âœ… è·¯ç”±å®Œæˆ: åˆ›å»ºäº† {len(send_list)} ä¸ª Send å¯¹è±¡")
    return send_list  # è¿”å› Send å¯¹è±¡åˆ—è¡¨ï¼Œç”¨äºåŠ¨æ€è·¯ç”±åˆ°å¤šä¸ª Map èŠ‚ç‚¹å®ä¾‹

def process_sub_data(sub_data: List[str]) -> dict:
    """å¤„ç†å­ä»»åŠ¡æ•°æ®ï¼Œç”Ÿæˆä¸­é—´ç»“æœ"""
    word_count = {}
    total_chars = 0

    for doc in sub_data:
        # ç»Ÿè®¡è¯é¢‘
        words = re.findall(r'\b\w+\b', doc.lower())
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
        # ç»Ÿè®¡å­—ç¬¦æ•°
        total_chars += len(doc)

    return {
        "word_count": word_count,
        "doc_count": len(sub_data),
        "total_chars": total_chars,
        "unique_words": len(word_count)
    }

def map_node(state: MapState):
    """Map èŠ‚ç‚¹å‡½æ•°ï¼Œè¾“å…¥çŠ¶æ€ä¸º MapState"""
    sub_data = state["sub_data"]  # ä»çŠ¶æ€ä¸­è·å–å­ä»»åŠ¡æ•°æ®
    print(f"ğŸ”§ Map èŠ‚ç‚¹: å¼€å§‹å¤„ç† {len(sub_data)} ä¸ªæ–‡æ¡£")

    intermediate_result = process_sub_data(sub_data)  # å¤„ç†å­ä»»åŠ¡æ•°æ®ï¼Œç”Ÿæˆä¸­é—´ç»“æœ

    print(f"âœ… Map èŠ‚ç‚¹: å¤„ç†å®Œæˆï¼Œæ‰¾åˆ° {intermediate_result['unique_words']}ä¸ªä¸åŒå•è¯")

    return {"intermediate_results": [intermediate_result]}  # è¿”å›ä¸­é—´ç»“æœï¼Œç”¨äºåç»­ Reduce é˜¶æ®µèšåˆ

def aggregate_results(intermediate_results: List[dict]) -> dict:
    """èšåˆä¸­é—´ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ"""
    global_word_count = {}
    total_docs = 0
    total_chars = 0

    for result in intermediate_results:
        total_docs += result["doc_count"]
        total_chars += result["total_chars"]

        # åˆå¹¶è¯é¢‘ç»Ÿè®¡
        for word, count in result["word_count"].items():
            global_word_count[word] = global_word_count.get(word, 0) + count

    # æ‰¾å‡ºæœ€é«˜é¢‘å’Œæœ€ä½é¢‘çš„è¯
    if global_word_count:
        sorted_words = sorted(global_word_count.items(), key=lambda x: x[1],
reverse=True)
        most_common = sorted_words[0]
        least_common = sorted_words[-1]
    else:
        most_common = ("", 0)
        least_common = ("", 0)

    return {
        "total_documents": total_docs,
        "total_characters": total_chars,
        "total_unique_words": len(global_word_count),
        "total_words": sum(global_word_count.values()),
        "most_common_word": most_common,
        "least_common_word": least_common,
        "word_distribution": dict(sorted_words[:10])  # åªä¿ç•™å‰10ä¸ªé«˜é¢‘è¯
    }

def reduce_node(state: OverallState):
    """Reduce èŠ‚ç‚¹å‡½æ•°ï¼Œè¾“å…¥çŠ¶æ€ä¸º OverallState"""
    intermediate_results = state["intermediate_results"]  # ä»çŠ¶æ€ä¸­è·å– Map é˜¶æ®µç”Ÿæˆçš„ä¸­é—´ç»“æœåˆ—è¡¨

    print(f"ğŸ”„ Reduce èŠ‚ç‚¹: æ±‡èš {len(intermediate_results)} ä¸ªä¸­é—´ç»“æœ")

    final_result = aggregate_results(intermediate_results)  # èšåˆä¸­é—´ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ

    print(f"âœ… Reduce å®Œæˆ: æ±‡æ€»äº† {final_result['total_documents']} ä¸ªæ–‡æ¡£")

    return {"final_result": final_result}  # è¿”å›æœ€ç»ˆç»“æœ

# æ„å»º MapReduce å›¾
print("ğŸ—ï¸ æ„å»ºæ ‡å‡† MapReduce å›¾...")
builder = StateGraph(OverallState)

# æ·»åŠ èŠ‚ç‚¹
builder.add_node("split_node", split_input_data)
builder.add_node("map_node", map_node)
builder.add_node("reduce_node", reduce_node)

# è¿æ¥ MapReduce æµç¨‹ä¸­çš„èŠ‚ç‚¹å’Œè¾¹
builder.add_edge(START, "split_node")

# å…³é”®ä¿®æ­£ï¼šåˆ†ç¦»æ•°æ®åˆ†å‰²å’Œä»»åŠ¡è·¯ç”±
# åˆ†å‰²èŠ‚ç‚¹ -> Map èŠ‚ç‚¹ (æ¡ä»¶è¾¹, ä½¿ç”¨ä¸“é—¨çš„è·¯ç”±å‡½æ•°)
builder.add_conditional_edges("split_node", route_to_map_nodes, ["map_node"])

# Map èŠ‚ç‚¹ -> Reduce èŠ‚ç‚¹ (æ™®é€šè¾¹)
builder.add_edge("map_node", "reduce_node")

# Reduce èŠ‚ç‚¹ -> END (æ™®é€šè¾¹)
builder.add_edge("reduce_node", END)

mapreduce_graph = builder.compile()
print("âœ… å›¾æ„å»ºå®Œæˆï¼")

# æµ‹è¯•æ•°æ®ï¼šæ¨¡æ‹Ÿå¤§è§„æ¨¡æ–‡æ¡£æ•°æ®
large_documents = [
    "LangGraph is a powerful framework for building AI agent systems with complex workflows.",
    "The framework provides comprehensive state management and advanced flow control capabilities.",
    "Parallel processing in LangGraph enables efficient task execution and resource utilization.",
    "MapReduce pattern helps process large datasets effectively using distributed computing principles.",
    "AI agents can use various tools and manage complex workflows with sophisticated coordination.",
    "State management is crucial for building reliable and scalable distributed systems.",
    "LangGraph supports dynamic branching with Send API for flexible workflowdesign.",
    "Concurrent execution improves overall system performance and throughput significantly.",
    "The Send API enables dynamic task distribution and parallel processing capabilities.",
    "Reducer functions ensure safe concurrent state updates in multi-threadedenvironments.",
    "Graph-based workflows provide clear visualization and better debugging capabilities.",
    "Advanced error handling and retry mechanisms ensure robust system operation."
]

print("\n=== ğŸš€ MapReduce å¤§è§„æ¨¡æ–‡æ¡£å¤„ç†æ¼”ç¤º ===")
print(f"ğŸ“„ è¾“å…¥æ–‡æ¡£æ•°é‡: {len(large_documents)}")
print(f"ğŸ“Š ä½¿ç”¨ Send API å®ç°åŠ¨æ€ä»»åŠ¡åˆ†å‘")
print(f"ğŸ”„ MapReduce æµç¨‹: åˆ†å‰² -> å¹¶è¡Œæ˜ å°„ -> å½’çº¦")
print("\n" + "="*60)

# æ‰§è¡Œ MapReduce æµç¨‹
result = mapreduce_graph.invoke({
    "large_input_data": large_documents,
    "sub_datasets": [],
    "intermediate_results": [],
    "final_result": {}
})

print("="*60)
print("\n=== âœ¨ MapReduce å¤„ç†ç»“æœ ===")
final_result = result["final_result"]
print(f"ğŸ“Š æ€»æ–‡æ¡£æ•°: {final_result['total_documents']}")
print(f"ğŸ“ æ€»å­—ç¬¦æ•°: {final_result['total_characters']}")
print(f"ğŸ”¤ ä¸åŒå•è¯æ•°: {final_result['total_unique_words']}")
print(f"ğŸ”¢ æ€»å•è¯æ•°: {final_result['total_words']}")
print(f"ğŸ† æœ€é«˜é¢‘è¯: '{final_result['most_common_word'][0]}' ({final_result['most_common_word'][1]} æ¬¡)")
print(f"ğŸ¥‰ æœ€ä½é¢‘è¯: '{final_result['least_common_word'][0]}' ({final_result['least_common_word'][1]} æ¬¡)")

print(f"\nğŸ“ˆ é«˜é¢‘è¯æ±‡ TOP 10:")
for word, count in final_result['word_distribution'].items():
    print(f"  ğŸ“Œ {word}: {count}")